{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Integrity Check ---\n",
      "✅ Confirmed: There are no common customer IDs between the two files.\n",
      "We will proceed by creating a simulated dataset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the original datasets\n",
    "social_profiles_df = pd.read_csv('customer_social_profiles.csv')\n",
    "transactions_df = pd.read_csv('customer_transactions.csv')\n",
    "\n",
    "# Correctly convert key columns to string type for comparison\n",
    "social_profiles_df['customer_id_new'] = social_profiles_df['customer_id_new'].astype(str)\n",
    "transactions_df['customer_id_legacy'] = transactions_df['customer_id_legacy'].astype(str)\n",
    "\n",
    "# Check for common IDs\n",
    "social_ids = set(social_profiles_df['customer_id_new'])\n",
    "transaction_ids = set(transactions_df['customer_id_legacy'])\n",
    "common_ids = social_ids.intersection(transaction_ids)\n",
    "\n",
    "print(\"--- Data Integrity Check ---\")\n",
    "if not common_ids:\n",
    "    print(\"✅ Confirmed: There are no common customer IDs between the two files.\")\n",
    "    print(\"We will proceed by creating a simulated dataset.\\n\")\n",
    "else:\n",
    "    print(f\"Found {len(common_ids)} common IDs. Direct merge is possible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Simulated dataset with intentional patterns created successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>social_media_platform</th>\n",
       "      <th>engagement_score</th>\n",
       "      <th>purchase_interest_score</th>\n",
       "      <th>review_sentiment</th>\n",
       "      <th>product</th>\n",
       "      <th>engagement_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A178</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>74</td>\n",
       "      <td>4.9</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A190</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>82</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Sports</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A150</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>96</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A162</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>89</td>\n",
       "      <td>2.6</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Home Goods</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A197</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>92</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Books</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id social_media_platform  engagement_score  \\\n",
       "0        A178              LinkedIn                74   \n",
       "1        A190               Twitter                82   \n",
       "2        A150              Facebook                96   \n",
       "3        A162               Twitter                89   \n",
       "4        A197               Twitter                92   \n",
       "\n",
       "   purchase_interest_score review_sentiment      product engagement_level  \n",
       "0                      4.9         Positive  Electronics           Medium  \n",
       "1                      4.8          Neutral       Sports             High  \n",
       "2                      1.6         Positive     Clothing             High  \n",
       "3                      2.6         Positive   Home Goods             High  \n",
       "4                      2.3          Neutral        Books             High  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We'll use the social profiles as our base\n",
    "merged_df = social_profiles_df.copy()\n",
    "\n",
    "# --- NEW: Define a function to create logical patterns ---\n",
    "def assign_product(row):\n",
    "    if row['social_media_platform'] == 'LinkedIn' and row['purchase_interest_score'] > 4.0:\n",
    "        return 'Electronics'\n",
    "    elif (row['social_media_platform'] == 'TikTok' or row['social_media_platform'] == 'Facebook') and row['engagement_score'] > 75:\n",
    "        return 'Clothing'\n",
    "    elif row['review_sentiment'] == 'Positive' and row['engagement_score'] > 70:\n",
    "        return 'Home Goods'\n",
    "    elif row['purchase_interest_score'] < 2.5:\n",
    "        return 'Books'\n",
    "    else:\n",
    "        return 'Sports'\n",
    "\n",
    "# Apply the function to create the target variable\n",
    "merged_df['product'] = merged_df.apply(assign_product, axis=1)\n",
    "\n",
    "# Engineer the 'engagement_level' feature\n",
    "bins = [0, 50, 80, 101]\n",
    "labels = ['Low', 'Medium', 'High']\n",
    "merged_df['engagement_level'] = pd.cut(merged_df['engagement_score'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Rename the customer_id column\n",
    "merged_df.rename(columns={'customer_id_new': 'customer_id'}, inplace=True)\n",
    "\n",
    "print(\"✅ Simulated dataset with intentional patterns created successfully.\")\n",
    "display(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean, merged dataset saved to 'merged_customer_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Save the final merged and cleaned dataframe\n",
    "merged_df.to_csv(\"merged_customer_data.csv\", index=False)\n",
    "\n",
    "print(\"Clean, merged dataset saved to 'merged_customer_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessed and ready for modeling.\n"
     ]
    }
   ],
   "source": [
    "# Select features, including our new engineered feature\n",
    "features = ['engagement_score', 'purchase_interest_score', 'review_sentiment', 'social_media_platform', 'engagement_level']\n",
    "target = 'product'\n",
    "\n",
    "X = merged_df[features]\n",
    "y = merged_df[target]\n",
    "\n",
    "# Encode categorical features\n",
    "X = pd.get_dummies(X, columns=['review_sentiment', 'social_media_platform', 'engagement_level'], drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data preprocessed and ready for modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Hyperparameter Tuning ---\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=1, n_estimators=50; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=1, n_estimators=50; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=1, n_estimators=50; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=1, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=1, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=1, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=4, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=4, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=4, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=4, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=4, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, n_estimators=200; total time=   0.1s[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, n_estimators=100; total time=   0.1s\n",
      "\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=4, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=4, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=4, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=4, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=4, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=4, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=4, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=4, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=4, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=4, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, n_estimators=200; total time=   0.2s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=4, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=4, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=4, n_estimators=200; total time=   0.1s\n",
      "\n",
      "--- Tuning Complete ---\n",
      "Best parameters found:  {'class_weight': 'balanced', 'max_depth': 10, 'min_samples_leaf': 1, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Set up the grid search with 3-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "print(\"--- Starting Hyperparameter Tuning ---\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n--- Tuning Complete ---\")\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Model Accuracy: 0.77\n",
      "Tuned Model F1-Score: 0.74\n",
      "\n",
      "--- Tuned Model Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Books       0.75      0.75      0.75         4\n",
      "    Clothing       0.86      0.86      0.86         7\n",
      " Electronics       1.00      1.00      1.00         1\n",
      "  Home Goods       0.67      0.29      0.40         7\n",
      "      Sports       0.75      1.00      0.86        12\n",
      "\n",
      "    accuracy                           0.77        31\n",
      "   macro avg       0.80      0.78      0.77        31\n",
      "weighted avg       0.76      0.77      0.74        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and F1-score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Tuned Model Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Tuned Model F1-Score: {f1:.2f}\\n\")\n",
    "\n",
    "# Print the classification report\n",
    "y_pred_labels = le.inverse_transform(y_pred)\n",
    "y_test_labels = le.inverse_transform(y_test)\n",
    "\n",
    "print(\"--- Tuned Model Classification Report ---\")\n",
    "print(classification_report(y_test_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Model Accuracy: 0.77\n",
      "Tuned Model F1-Score: 0.74\n",
      "\n",
      "--- Tuned Model Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Books       0.75      0.75      0.75         4\n",
      "    Clothing       0.86      0.86      0.86         7\n",
      " Electronics       1.00      1.00      1.00         1\n",
      "  Home Goods       0.67      0.29      0.40         7\n",
      "      Sports       0.75      1.00      0.86        12\n",
      "\n",
      "    accuracy                           0.77        31\n",
      "   macro avg       0.80      0.78      0.77        31\n",
      "weighted avg       0.76      0.77      0.74        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and F1-score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Tuned Model Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Tuned Model F1-Score: {f1:.2f}\\n\")\n",
    "\n",
    "# Print the classification report\n",
    "y_pred_labels = le.inverse_transform(y_pred)\n",
    "y_test_labels = le.inverse_transform(y_test)\n",
    "\n",
    "print(\"--- Tuned Model Classification Report ---\")\n",
    "print(classification_report(y_test_labels, y_pred_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
